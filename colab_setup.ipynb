{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Social Media Post Scraper - Google Colab Setup\n\nThis notebook sets up and runs the Chinese Social Media scraper in Google Colab.\n\n## ‚ö†Ô∏è Important Limitations\n\n‚úÖ **What WILL work:**\n- Douyin post scraping (including comments)\n- Weibo post scraping\n- Basic Weixin scraping (title, content, publish date)\n- Database storage and Excel export\n\n‚ùå **What WON'T work:**\n- scrape_weixin_post_ui. py (requires Windows desktop automation)\n- Advanced Weixin metrics (like/share/comment counts)"
      ],
      "metadata": {"id": "title"}
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 1: Install System Dependencies"],
      "metadata": {"id": "step1"}
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n!apt-get install -y tesseract-ocr tesseract-ocr-chi-sim tesseract-ocr-chi-tra tesseract-ocr-eng\nprint('‚úÖ System dependencies installed!')"
      ],
      "metadata": {"id": "install_deps"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 2: Clone the Repository"],
      "metadata": {"id": "step2"}
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/pwklam/scrape_chinese_social_media.git\n%cd scrape_chinese_social_media\n!ls -la\nprint('\\n‚úÖ Repository cloned successfully!')"
      ],
      "metadata": {"id": "clone_repo"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 3: Install Python Dependencies"],
      "metadata": {"id": "step3"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ALL required Python packages from requirements.txt\n!pip install -q -r requirements.txt\nprint('‚úÖ All Python packages from requirements. txt installed!')"
      ],
      "metadata": {"id": "install_python"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 4: Install Playwright Browsers"],
      "metadata": {"id": "step4"}
    },
    {
      "cell_type": "code",
      "source": [
        "! playwright install chromium\n!playwright install-deps chromium\nprint('\\n‚úÖ Playwright browsers installed!')"
      ],
      "metadata": {"id": "install_playwright"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 5: Update Configuration for Colab Environment"],
      "metadata": {"id": "step5"}
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = '''import os\\n\\n# Tesseract OCR configuration (Colab path)\\ntesseract_cmd = '/usr/bin/tesseract'\\n\\n# Database configuration\\nDATABASE_PATH = 'data.db'\\n\\n# Scraping configuration\\nDOUYIN_TIMEOUT = 30000\\nWEIBO_TIMEOUT = 30000\\nWEIXIN_TIMEOUT = 30000\\n\\n# Comment scraping configuration\\nMAX_COMMENTS = 20\\nSCROLL_ATTEMPTS = 3\\n'''\n\nwith open('config.py', 'w', encoding='utf-8') as f:\n    f.write(config_content)\n\nprint('‚úÖ Configuration updated for Colab environment!')"
      ],
      "metadata": {"id": "update_config"},
      "execution_count": null,
      "outputs": []
    },
  {
  "cell_type": "markdown",
  "source": ["## Step 6: Verify URLs from GitHub\n\nThe urls.txt file is automatically cloned from your GitHub repository. "],
  "metadata": {"id": "step6"}
  },
  {
  "cell_type": "code",
  "source": [
    "# Display URLs from GitHub repository\nprint('‚úÖ Using urls.txt from GitHub repository')\nprint('\\nüìÑ Current URLs:')\n!cat urls.txt\n\nprint('\\nüí° To change URLs:')\nprint('   1. Edit urls.txt on GitHub: https://github.com/pwklam/scrape_chinese_social_media/blob/main/urls.txt')\nprint('   2. Re-run Step 2 to pull the latest version')\nprint('\\n   OR add URLs below:')\n\n# Uncomment to add more URLs temporarily\n# additional_urls = '''\\nhttps://weibo.com/YOUR_URL\\n'''\n# with open('urls. txt', 'a', encoding='utf-8') as f:\n#     f.write(additional_urls)\n# print('\\n‚úÖ Additional URLs added!')\n# ! cat urls.txt"
  ],
  "metadata": {"id": "verify_urls"},
  "execution_count": null,
  "outputs": []
  }
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 7: View/Edit urls.txt"],
      "metadata": {"id": "step7"}
    },
    {
      "cell_type": "code",
      "source": ["!cat urls.txt"],
      "metadata": {"id": "view_urls"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 8: Run the Scraper"],
      "metadata": {"id": "step8"}
    },
    {
      "cell_type": "code",
      "source": [
        "! python main.py\nprint('\\n‚úÖ Scraping completed!')"
      ],
      "metadata": {"id": "run_scraper"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 9: Export Data to Excel"],
      "metadata": {"id": "step9"}
    },
    {
      "cell_type": "code",
      "source": [
        "! python export_excel_data.py\nprint('\\n‚úÖ Data exported to data.xlsx!')"
      ],
      "metadata": {"id": "export_excel"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 10: Preview the Data"],
      "metadata": {"id": "step10"}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n\ntry:\n    df = pd. read_excel('data.xlsx')\n    print(f'‚úÖ Total posts scraped: {len(df)}')\n    print('\\nüìä First few rows:')\n    display(df.head())\n    print('\\nüìã Columns:')\n    print(df.columns.tolist())\nexcept FileNotFoundError:\n    print('‚ö†Ô∏è No data. xlsx file found.')\nexcept Exception as e:\n    print(f'‚ùå Error: {e}')"
      ],
      "metadata": {"id": "preview_data"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 11: Download Results"],
      "metadata": {"id": "step11"}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\nimport os\n\nif os.path.exists('data.xlsx'):\n    files.download('data.xlsx')\n    print('‚úÖ Downloaded: data.xlsx')\nelse:\n    print('‚ö†Ô∏è data.xlsx not found')\n\nif os.path.exists('data. db'):\n    files.download('data.db')\n    print('‚úÖ Downloaded: data.db')\nelse:\n    print('‚ö†Ô∏è data.db not found')\n\nif os.path.exists('app.log'):\n    files.download('app.log')\n    print('‚úÖ Downloaded: app. log')\nelse:\n    print('‚ö†Ô∏è app.log not found')"
      ],
      "metadata": {"id": "download_files"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Troubleshooting\n\n### Common Issues:\n\n1. **Login Required**: Weibo may require login (won't work in headless Colab)\n2. **Timeouts**: Increase timeout values in config.py\n3. **No Data**: Check app.log for errors\n\n### Platform Notes:\n\n- **Douyin**: Includes up to 20 comments\n- **Weibo**: Includes post metrics and comments\n- **Weixin**: Basic info only (no advanced metrics in Colab)\n\n---\n\n**Repository**: [pwklam/scrape_chinese_social_media](https://github.com/pwklam/scrape_chinese_social_media)"
      ],
      "metadata": {"id": "notes"}
    }
  ]
}
