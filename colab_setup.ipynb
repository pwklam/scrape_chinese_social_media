{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Social Media Post Scraper - Google Colab Setup\n\nThis notebook sets up and runs the Chinese Social Media scraper in Google Colab.\n\n## ‚ö†Ô∏è Important Limitations\n\n‚úÖ **What WILL work:**\n- Douyin post scraping (including comments)\n- Weibo post scraping (including comments)\n- Basic Weixin scraping (title, content, publish date)\n- Database storage and Excel export\n\n‚ùå **What WON'T work:**\n- scrape_weixin_post_ui.py (requires Windows desktop automation)\n- Advanced Weixin metrics (like/share/comment counts)\n- Login-protected content (Weibo may require login)"
      ],
      "metadata": {"id": "title"}
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 1: Install System Dependencies"],
      "metadata": {"id": "step1"}
    },
    {
      "cell_type": "code",
      "source": [
        "! apt-get update\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-sim tesseract-ocr-chi-tra tesseract-ocr-eng\n",
        "print('‚úÖ System dependencies installed!')"
      ],
      "metadata": {"id": "install_deps"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 2: Clone the Repository"],
      "metadata": {"id": "step2"}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Remove old repository if exists (prevents duplicate paths)\n",
        "if os.path.exists('/content/scrape_chinese_social_media'):\n",
        "    print('üóëÔ∏è Removing old repository.. .')\n",
        "    ! rm -rf /content/scrape_chinese_social_media\n",
        "\n",
        "# Clone fresh copy\n",
        "!git clone https://github.com/pwklam/scrape_chinese_social_media. git\n",
        "\n",
        "# Change to repository directory\n",
        "%cd /content/scrape_chinese_social_media\n",
        "\n",
        "# Verify location and files\n",
        "print('\\nüìÇ Current directory:')\n",
        "!pwd\n",
        "print('\\nüìÑ Python files:')\n",
        "!ls -la *.py\n",
        "print('\\n‚úÖ Repository cloned successfully!')"
      ],
      "metadata": {"id": "clone_repo"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 3: Install Python Dependencies"],
      "metadata": {"id": "step3"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ALL required Python packages from requirements.txt\n",
        "! pip install -q -r requirements.txt\n",
        "print('‚úÖ All Python packages from requirements.txt installed!')"
      ],
      "metadata": {"id": "install_python"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 4: Install Playwright Browsers"],
      "metadata": {"id": "step4"}
    },
    {
      "cell_type": "code",
      "source": [
        "! playwright install chromium\n",
        "! playwright install-deps chromium\n",
        "print('\\n‚úÖ Playwright browsers installed!')"
      ],
      "metadata": {"id": "install_playwright"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 5: Update Configuration for Colab Environment"],
      "metadata": {"id": "step5"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix config.py with ALL required attributes for Colab\n",
        "config_content = '''import os\n",
        "\n",
        "# Database configuration\n",
        "db_name = \"data.db\"\n",
        "table_name = \"posts\"\n",
        "\n",
        "# Tesseract OCR configuration (Colab path)\n",
        "tesseract_cmd = \"/usr/bin/tesseract\"\n",
        "\n",
        "# Scraping configuration\n",
        "DOUYIN_TIMEOUT = 30000\n",
        "WEIBO_TIMEOUT = 30000\n",
        "WEIXIN_TIMEOUT = 30000\n",
        "\n",
        "# Comment scraping configuration\n",
        "MAX_COMMENTS = 20\n",
        "SCROLL_ATTEMPTS = 3\n",
        "\n",
        "# Export Excel path\n",
        "export_excel_path = \"data.xlsx\"\n",
        "'''\n",
        "\n",
        "with open('config.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print('‚úÖ Configuration updated for Colab environment!')\n",
        "print('\\nüìÑ Config contents:')\n",
        "! cat config.py"
      ],
      "metadata": {"id": "update_config"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Verify URLs from GitHub\n",
        "\n",
        "The urls.txt file is automatically cloned from your GitHub repository in Step 2.\n",
        "\n",
        "**To update URLs:**\n",
        "1. Edit urls.txt on GitHub: https://github.com/pwklam/scrape_chinese_social_media/blob/main/urls.txt\n",
        "2. Re-run Step 2 to clone the latest version\n",
        "\n",
        "**Or** use the code below to add URLs temporarily (without committing to GitHub)."
      ],
      "metadata": {"id": "step6"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Display URLs from GitHub repository\n",
        "print('‚úÖ Using urls.txt from GitHub repository')\n",
        "print('\\nüìÑ Current URLs in urls.txt:')\n",
        "!cat urls. txt\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üí° How to manage URLs:')\n",
        "print('='*60)\n",
        "print('1. Edit on GitHub: https://github.com/pwklam/scrape_chinese_social_media/blob/main/urls.txt')\n",
        "print('2. Then re-run Step 2 to pull latest changes')\n",
        "print('\\n   OR temporarily add URLs below (uncomment the code):')\n",
        "print('='*60)\n",
        "\n",
        "# Uncomment the lines below to add more URLs temporarily\n",
        "# additional_urls = '''\\nhttps://weibo.com/YOUR_WEIBO_URL\n",
        "# https://www.douyin.com/YOUR_DOUYIN_URL\n",
        "# '''\n",
        "\n",
        "# with open('urls.txt', 'a', encoding='utf-8') as f:\n",
        "#     f. write(additional_urls)\n",
        "\n",
        "# print('\\n‚úÖ Additional URLs added!')\n",
        "# print('\\nüìÑ Updated URLs:')\n",
        "# ! cat urls.txt"
      ],
      "metadata": {"id": "verify_urls"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 7: Run the Scraper"],
      "metadata": {"id": "step7"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we're in the right directory\n",
        "%cd /content/scrape_chinese_social_media\n",
        "\n",
        "print('='*60)\n",
        "print('üöÄ Starting scraper...')\n",
        "print('='*60 + '\\n')\n",
        "\n",
        "! python main.py\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('‚úÖ Scraping completed!')\n",
        "print('='*60)\n",
        "\n",
        "# Verify results\n",
        "import os\n",
        "import sqlite3\n",
        "\n",
        "if os. path.exists('data.db'):\n",
        "    conn = sqlite3.connect('data. db')\n",
        "    cursor = conn.cursor()\n",
        "    try:\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM posts\")\n",
        "        count = cursor. fetchone()[0]\n",
        "        print(f'\\nüìä Total posts in database: {count}')\n",
        "    except Exception as e:\n",
        "        print(f'\\n‚ö†Ô∏è Database error: {e}')\n",
        "    conn.close()\n",
        "else:\n",
        "    print('\\n‚ùå No database created - check errors above')"
      ],
      "metadata": {"id": "run_scraper"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 8: Export Data to Excel"],
      "metadata": {"id": "step8"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we're in the right directory\n",
        "%cd /content/scrape_chinese_social_media\n",
        "\n",
        "# Verify export script exists\n",
        "import os\n",
        "if os.path.exists('export_excel_data.py'):\n",
        "    print('‚úÖ Found export_excel_data.py')\n",
        "    ! python export_excel_data.py\n",
        "    \n",
        "    if os.path.exists('data. xlsx'):\n",
        "        print('\\n‚úÖ Data exported to data.xlsx!')\n",
        "    else:\n",
        "        print('\\n‚ö†Ô∏è export_excel_data.py ran but no data.xlsx created')\n",
        "else:\n",
        "    print('‚ùå export_excel_data. py not found!')"
      ],
      "metadata": {"id": "export_excel"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 9: Preview the Data"],
      "metadata": {"id": "step9"}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel('data.xlsx')\n",
        "    print(f'‚úÖ Total posts scraped: {len(df)}')\n",
        "    print('\\nüìä First few rows:')\n",
        "    display(df.head())\n",
        "    print('\\nüìã Columns:')\n",
        "    print(df.columns.tolist())\n",
        "    \n",
        "    # Check for comments\n",
        "    if 'comments' in df.columns and not df. empty:\n",
        "        comments_data = df['comments'].iloc[0]\n",
        "        if comments_data and str(comments_data) != 'nan':\n",
        "            try:\n",
        "                comments = json.loads(comments_data)\n",
        "                print(f'\\nüí¨ Found {len(comments)} comments in first post')\n",
        "                if comments:\n",
        "                    print('\\nüìù Sample comment:')\n",
        "                    print(json.dumps(comments[0], indent=2, ensure_ascii=False))\n",
        "            except:\n",
        "                print('\\n‚ö†Ô∏è Comments field exists but could not parse')\n",
        "        else:\n",
        "            print('\\n‚ö†Ô∏è No comments found in the scraped post')\n",
        "            \n",
        "except FileNotFoundError:\n",
        "    print('‚ö†Ô∏è No data.xlsx file found.')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Error: {e}')"
      ],
      "metadata": {"id": "preview_data"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 10: Download Results"],
      "metadata": {"id": "step10"}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "if os.path.exists('data. xlsx'):\n",
        "    files.download('data.xlsx')\n",
        "    print('‚úÖ Downloaded: data.xlsx')\n",
        "else:\n",
        "    print('‚ö†Ô∏è data.xlsx not found')\n",
        "\n",
        "if os. path.exists('data.db'):\n",
        "    files.download('data.db')\n",
        "    print('‚úÖ Downloaded: data.db')\n",
        "else:\n",
        "    print('‚ö†Ô∏è data. db not found')\n",
        "\n",
        "if os.path.exists('app.log'):\n",
        "    files.download('app. log')\n",
        "    print('‚úÖ Downloaded: app.log')\n",
        "else:\n",
        "    print('‚ö†Ô∏è app.log not found')"
      ],
      "metadata": {"id": "download_files"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "1. **Login Required**: Weibo may require login (won't work in headless Colab)\n",
        "   - Solution: Run locally with `headless=False` in scraper code\n",
        "\n",
        "2. **No Comments Scraped**: Comments section didn't load or requires login\n",
        "   - Check if the post actually has comments on Weibo\n",
        "   - Weibo may block automated access\n",
        "\n",
        "3. **Timeouts**: Page took too long to load\n",
        "   - Increase timeout values in config.py\n",
        "\n",
        "4. **No Data**: Check app.log for errors\n",
        "   - Database may be empty if scraping failed\n",
        "\n",
        "### Platform Notes:\n",
        "\n",
        "- **Douyin**: Includes up to 20 comments\n",
        "- **Weibo**: Includes post metrics and up to 20 comments\n",
        "- **Weixin**: Basic info only (no advanced metrics in Colab)\n",
        "\n",
        "### Success Indicators:\n",
        "\n",
        "Look for these messages in Step 7 output:\n",
        "```\n",
        "üöÄ Launching Playwright browser...\n",
        "üîç Page content loaded, starting data extraction...\n",
        "üíæ Inserting scraped data into the database...\n",
        "üîé Scraping comments for: [URL]\n",
        "‚úÖ Successfully scraped X comments\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Repository**: [pwklam/scrape_chinese_social_media](https://github. com/pwklam/scrape_chinese_social_media)"
      ],
      "metadata": {"id": "notes"}
    }
  ]
}
