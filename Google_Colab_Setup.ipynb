{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Social Media Post Scraper - Google Colab Setup\n",
        "\n",
        "This notebook sets up and runs the Chinese Social Media scraper in Google Colab.\n",
        "\n",
        "## ‚ö†Ô∏è Important Limitations in Google Colab\n",
        "\n",
        "‚úÖ **What WILL work:**\n",
        "- Douyin post scraping (including comments)\n",
        "- Weibo post scraping\n",
        "- Basic Weixin scraping (title, content, publish date)\n",
        "- Database storage and Excel export\n",
        "\n",
        "‚ùå **What WON'T work:**\n",
        "- `scrape_weixin_post_ui.py` (requires Windows desktop automation)\n",
        "- Advanced Weixin metrics (like/share/comment counts - requires Windows UI automation)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install System Dependencies"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Tesseract OCR and language packs\n",
        "! apt-get update\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-sim tesseract-ocr-chi-tra tesseract-ocr-eng\n",
        "\n",
        "print(\"‚úÖ System dependencies installed! \")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Clone the Repository"
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone your repository\n",
        "! git clone https://github.com/pwklam/scrape_chinese_social_media.git\n",
        "%cd scrape_chinese_social_media\n",
        "\n",
        "# List files to verify\n",
        "!ls -la\n",
        "\n",
        "print(\"\\n‚úÖ Repository cloned successfully!\")"
      ],
      "metadata": {
        "id": "clone_repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Install Python Dependencies"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required Python packages\n",
        "!pip install -q playwright pandas openpyxl pytesseract Pillow\n",
        "\n",
        "print(\"‚úÖ Python packages installed!\")"
      ],
      "metadata": {
        "id": "install_python"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Install Playwright Browsers"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Playwright and its browser dependencies\n",
        "!playwright install chromium\n",
        "! playwright install-deps chromium\n",
        "\n",
        "print(\"\\n‚úÖ Playwright browsers installed!\")"
      ],
      "metadata": {
        "id": "install_playwright"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Update Configuration for Colab Environment"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update config. py to use Colab's Tesseract path\n",
        "config_content = '''import os\n",
        "\n",
        "# Tesseract OCR configuration (Colab path)\n",
        "tesseract_cmd = '/usr/bin/tesseract'\n",
        "\n",
        "# Database configuration\n",
        "DATABASE_PATH = 'data. db'\n",
        "\n",
        "# Scraping configuration\n",
        "DOUYIN_TIMEOUT = 30000  # 30 seconds\n",
        "WEIBO_TIMEOUT = 30000\n",
        "WEIXIN_TIMEOUT = 30000\n",
        "\n",
        "# Comment scraping configuration\n",
        "MAX_COMMENTS = 20\n",
        "SCROLL_ATTEMPTS = 3\n",
        "'''\n",
        "\n",
        "with open('config.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"‚úÖ Configuration updated for Colab environment!\")"
      ],
      "metadata": {
        "id": "update_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Add Your URLs\n",
        "\n",
        "Edit the `urls.txt` file with your target URLs (one per line)."
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Create urls.txt with sample URLs\n",
        "# REPLACE THESE WITH YOUR ACTUAL URLS!\n",
        "\n",
        "urls_content = '''# Add your URLs here, one per line\n",
        "# Douyin example:\n",
        "# https://www.douyin.com/video/1234567890123456789\n",
        "\n",
        "# Weibo example:\n",
        "# https://weibo.com/1234567890/Abcdefghijk\n",
        "\n",
        "# Weixin example:\n",
        "# https://mp.weixin.qq.com/s/abcdefghijklmnopqrstuvwxyz\n",
        "'''\n",
        "\n",
        "with open('urls.txt', 'w', encoding='utf-8') as f:\n",
        "    f. write(urls_content)\n",
        "\n",
        "print(\"‚úÖ urls.txt created! \")\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Edit the urls.txt file above with your actual URLs before running the scraper!\")\n",
        "print(\"\\nYou can edit it in the next cell. \")"
      ],
      "metadata": {
        "id": "create_urls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: View/Edit urls.txt\n",
        "\n",
        "Use this cell to view and edit your URLs:"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View current urls.txt content\n",
        "!cat urls.txt"
      ],
      "metadata": {
        "id": "view_urls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Run the Scraper\n",
        "\n",
        "This will scrape all URLs from `urls.txt` and save data to `data.db`."
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main scraper\n",
        "!python main.py\n",
        "\n",
        "print(\"\\n‚úÖ Scraping completed!  Check the output above for details.\")"
      ],
      "metadata": {
        "id": "run_scraper"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Export Data to Excel"
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export database to Excel\n",
        "!python export_excel_data.py\n",
        "\n",
        "print(\"\\n‚úÖ Data exported to data.xlsx!\")"
      ],
      "metadata": {
        "id": "export_excel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Preview the Data"
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the scraped data\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel('data.xlsx')\n",
        "    print(f\"Total posts scraped: {len(df)}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    display(df.head())\n",
        "    \n",
        "    print(\"\\nColumn summary:\")\n",
        "    print(df.info())\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è  No data. xlsx file found. Make sure scraping completed successfully.\")"
      ],
      "metadata": {
        "id": "preview_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Download Results\n",
        "\n",
        "Download the database, Excel file, and log file to your local machine."
      ],
      "metadata": {
        "id": "step11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Download data. xlsx\n",
        "if os.path.exists('data.xlsx'):\n",
        "    files.download('data.xlsx')\n",
        "    print(\"‚úÖ Downloaded: data.xlsx\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  data.xlsx not found\")\n",
        "\n",
        "# Download data.db\n",
        "if os.path.exists('data.db'):\n",
        "    files.download('data. db')\n",
        "    print(\"‚úÖ Downloaded: data.db\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  data.db not found\")\n",
        "\n",
        "# Download app.log\n",
        "if os.path.exists('app.log'):\n",
        "    files.download('app. log')\n",
        "    print(\"‚úÖ Downloaded: app.log\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  app.log not found\")"
      ],
      "metadata": {
        "id": "download_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Additional Notes\n",
        "\n",
        "### Troubleshooting:\n",
        "\n",
        "1. **Timeouts**: If scraping times out, try increasing timeout values in `config.py`\n",
        "2. **Missing data**: Some platforms may block automated access - check `app.log` for details\n",
        "3.  **Empty results**: Verify your URLs are correct and accessible\n",
        "\n",
        "### Platform-Specific Notes:\n",
        "\n",
        "- **Douyin**: Includes up to 20 comments per post with author, content, timestamp, and likes\n",
        "- **Weibo**: Includes basic post metrics\n",
        "- **Weixin**: Only basic info (title, content, date) - advanced metrics require Windows UI automation\n",
        "\n",
        "### Re-running the Scraper:\n",
        "\n",
        "To scrape new URLs:\n",
        "1. Update `urls.txt` (Step 6-7)\n",
        "2. Re-run Steps 8-11\n",
        "\n",
        "---\n",
        "\n",
        "**Repository**: [pwklam/scrape_chinese_social_media](https://github. com/pwklam/scrape_chinese_social_media)"
      ],
      "metadata": {
        "id": "notes"
      }
    }
  ]
}
